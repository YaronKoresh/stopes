[build-system]
requires = ["setuptools>=61.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "stopes"
readme = "README.md"
authors = [{name = "Facebook AI Research"}]
requires-python = ">=3.9"
version = "2.2.1"
description = "A library for preparing data for machine translation research."
dependencies = [
  "hydra-core>=1.2.0",
  "joblib",
  "submitit>=1.4.5",
  "tqdm",
  "pyarrow>=16.1.0"
]
classifiers=[
    "License :: OSI Approved :: MIT License",
    "Topic :: Scientific/Engineering",
    "Development Status :: 4 - Beta",
]

[project.urls]
Source = "https://github.com/facebookresearch/stopes"
Tracker = "https://github.com/facebookresearch/stopes/issues"

[project.optional-dependencies]
text = [
    "numpy~=1.26.4",
    "pandas~=2.2.2",
    "scipy~=1.13.1",
    "torch~=2.3.1",
    "transformers~=4.41.2",
    "nltk~=3.8.1",
    "xxhash~=3.4.1",
    "sacremoses~=0.1.1",
    "sentencepiece~=0.2.0",
    "botok~=0.9.0",
    "emoji~=2.12.1",
    "fasttext~=0.9.3",
    "hasami~=0.0.1",
    "indic-nlp-library~=0.81",
    "khmer-nltk~=1.6",
    "laonlp~=1.2.0",
    "pythainlp==4.0.2",
    "sentence_splitter~=1.4",
]
speech = [
    "numpy~=1.26.4",
    "pandas~=2.2.2",
    "scipy~=1.13.1",
    "torch~=2.3.1",
    "transformers~=4.41.2",
    "nltk~=3.8.1",
    "xxhash~=3.4.1",
    "sacremoses~=0.1.1",
    "sentencepiece~=0.2.0",
    "demucs~=4.0.1",
    "encodec~=0.1.1",
    "ipapy~=0.0.9.0",
    "librosa~=0.10.2",
    "num2words~=0.5.13",
    "numba~=0.59.1",
    "phonemizer~=3.2.1",
    "syllables~=1.0.9",
    "tnkeeh~=0.0.9",
    "torchaudio~=2.3.1",
    "unidecode~=1.3.8",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper==20230314",
    "s3prl~=0.4.1",
    "sentence_transformers~=3.0.1",
]
training = [
    "fairscale~=0.4.13",
    "omegaconf~=2.3.0",
    "einops~=0.8.0",
]
evaluation = [
    "fairscale~=0.4.13",
    "omegaconf~=2.3.0",
    "einops~=0.8.0",
]
mining-cpu = [
    "numpy~=1.26.4",
    "pandas~=2.2.2",
    "scipy~=1.13.1",
    "torch~=2.3.1",
    "transformers~=4.41.2",
    "nltk~=3.8.1",
    "xxhash~=3.4.1",
    "sacremoses~=0.1.1",
    "sentencepiece~=0.2.0",
    "beautifulsoup4~=4.12.3",
    "func_argparse~=1.1.1",
    "requests~=2.32.3",
    "scikit-learn~=1.5.0",
    "faiss-cpu~=1.8.0",
]
mining-gpu = [
    "numpy~=1.26.4",
    "pandas~=2.2.2",
    "scipy~=1.13.1",
    "torch~=2.3.1",
    "transformers~=4.41.2",
    "nltk~=3.8.1",
    "xxhash~=3.4.1",
    "sacremoses~=0.1.1",
    "sentencepiece~=0.2.0",
    "beautifulsoup4~=4.12.3",
    "func_argparse~=1.1.1",
    "requests~=2.32.3",
    "scikit-learn~=1.5.0",
    "faiss @ https://github.com/YaronKoresh/faiss/releases/download/6470b8d/faiss-1.12.0-py3-none-manylinux_2_35_x86_64.whl",
]
inference-cpu = [
    "numpy~=1.26.4",
    "pandas~=2.2.2",
    "scipy~=1.13.1",
    "torch~=2.3.1",
    "transformers~=4.41.2",
    "nltk~=3.8.1",
    "xxhash~=3.4.1",
    "sacremoses~=0.1.1",
    "sentencepiece~=0.2.0",
    "botok~=0.9.0",
    "emoji~=2.12.1",
    "fasttext~=0.9.3",
    "hasami~=0.0.1",
    "indic-nlp-library~=0.81",
    "khmer-nltk~=1.6",
    "laonlp~=1.2.0",
    "pythainlp==4.0.2",
    "sentence_splitter~=1.4",
    "demucs~=4.0.1",
    "encodec~=0.1.1",
    "ipapy~=0.0.9.0",
    "librosa~=0.10.2",
    "num2words~=0.5.13",
    "numba~=0.59.1",
    "phonemizer~=3.2.1",
    "syllables~=1.0.9",
    "tnkeeh~=0.0.9",
    "torchaudio~=2.3.1",
    "unidecode~=1.3.8",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper==20230314",
    "s3prl~=0.4.1",
    "sentence_transformers~=3.0.1",
    "beautifulsoup4~=4.12.3",
    "func_argparse~=1.1.1",
    "requests~=2.32.3",
    "scikit-learn~=1.5.0",
    "faiss-cpu~=1.8.0",
    "fairseq2~=0.5.2",
    "sonar-space==0.2.*",
]
inference-gpu = [
    "numpy~=1.26.4",
    "pandas~=2.2.2",
    "scipy~=1.13.1",
    "torch~=2.3.1",
    "transformers~=4.41.2",
    "nltk~=3.8.1",
    "xxhash~=3.4.1",
    "sacremoses~=0.1.1",
    "sentencepiece~=0.2.0",
    "botok~=0.9.0",
    "emoji~=2.12.1",
    "fasttext~=0.9.3",
    "hasami~=0.0.1",
    "indic-nlp-library~=0.81",
    "khmer-nltk~=1.6",
    "laonlp~=1.2.0",
    "pythainlp==4.0.2",
    "sentence_splitter~=1.4",
    "demucs~=4.0.1",
    "encodec~=0.1.1",
    "ipapy~=0.0.9.0",
    "librosa~=0.10.2",
    "num2words~=0.5.13",
    "numba~=0.59.1",
    "phonemizer~=3.2.1",
    "syllables~=1.0.9",
    "tnkeeh~=0.0.9",
    "torchaudio~=2.3.1",
    "unidecode~=1.3.8",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper==20230314",
    "s3prl~=0.4.1",
    "sentence_transformers~=3.0.1",
    "beautifulsoup4~=4.12.3",
    "func_argparse~=1.1.1",
    "requests~=2.32.3",
    "scikit-learn~=1.5.0",
    "faiss @ https://github.com/YaronKoresh/faiss/releases/download/6470b8d/faiss-1.12.0-py3-none-manylinux_2_35_x86_64.whl",
    "fairseq2~=0.5.2",
    "sonar-space==0.2.*",
]
training-cpu = [
    "numpy~=1.26.4",
    "pandas~=2.2.2",
    "scipy~=1.13.1",
    "torch~=2.3.1",
    "transformers~=4.41.2",
    "nltk~=3.8.1",
    "xxhash~=3.4.1",
    "sacremoses~=0.1.1",
    "sentencepiece~=0.2.0",
    "botok~=0.9.0",
    "emoji~=2.12.1",
    "fasttext~=0.9.3",
    "hasami~=0.0.1",
    "indic-nlp-library~=0.81",
    "khmer-nltk~=1.6",
    "laonlp~=1.2.0",
    "pythainlp==4.0.2",
    "sentence_splitter~=1.4",
    "demucs~=4.0.1",
    "encodec~=0.1.1",
    "ipapy~=0.0.9.0",
    "librosa~=0.10.2",
    "num2words~=0.5.13",
    "numba~=0.59.1",
    "phonemizer~=3.2.1",
    "syllables~=1.0.9",
    "tnkeeh~=0.0.9",
    "torchaudio~=2.3.1",
    "unidecode~=1.3.8",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper==20230314",
    "s3prl~=0.4.1",
    "sentence_transformers~=3.0.1",
    "beautifulsoup4~=4.12.3",
    "func_argparse~=1.1.1",
    "requests~=2.32.3",
    "scikit-learn~=1.5.0",
    "faiss-cpu~=1.8.0",
    "fairseq2~=0.5.2",
    "sonar-space==0.2.*",
    "fairscale~=0.4.13",
    "omegaconf~=2.3.0",
    "einops~=0.8.0",
]
training-gpu = [
    "numpy~=1.26.4",
    "pandas~=2.2.2",
    "scipy~=1.13.1",
    "torch~=2.3.1",
    "transformers~=4.41.2",
    "nltk~=3.8.1",
    "xxhash~=3.4.1",
    "sacremoses~=0.1.1",
    "sentencepiece~=0.2.0",
    "botok~=0.9.0",
    "emoji~=2.12.1",
    "fasttext~=0.9.3",
    "hasami~=0.0.1",
    "indic-nlp-library~=0.81",
    "khmer-nltk~=1.6",
    "laonlp~=1.2.0",
    "pythainlp==4.0.2",
    "sentence_splitter~=1.4",
    "demucs~=4.0.1",
    "encodec~=0.1.1",
    "ipapy~=0.0.9.0",
    "librosa~=0.10.2",
    "num2words~=0.5.13",
    "numba~=0.59.1",
    "phonemizer~=3.2.1",
    "syllables~=1.0.9",
    "tnkeeh~=0.0.9",
    "torchaudio~=2.3.1",
    "unidecode~=1.3.8",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper==20230314",
    "s3prl~=0.4.1",
    "sentence_transformers~=3.0.1",
    "beautifulsoup4~=4.12.3",
    "func_argparse~=1.1.1",
    "requests~=2.32.3",
    "scikit-learn~=1.5.0",
    "faiss @ https://github.com/YaronKoresh/faiss/releases/download/6470b8d/faiss-1.12.0-py3-none-manylinux_2_35_x86_64.whl",
    "fairseq2~=0.5.2",
    "sonar-space==0.2.*",
    "fairscale~=0.4.13",
    "omegaconf~=2.3.0",
    "einops~=0.8.0",
]
dev = [
    "coverage[toml]~=7.5.4",
    "flit>=3.9.0",
    "mypy~=1.10.0",
    "pytest~=8.2.2",
    "pytest-asyncio~=0.23.7",
    "pytest-cov~=5.0.0",
    "ruff~=0.5.0",
    "types-emoji~=2.1.0.3",
    "types-PyYAML~=6.0.12",
    "types-requests~=2.32.0",
]

[tool.setuptools.packages.find]
where = ["."]
include = ["stopes"]

[tool.ruff]
line-length = 88
select = ["I"]

[tool.ruff.lint]
select = ["E", "F", "W", "C90", "I"]
ignore = []

[tool.mypy]
python_version = "3.9"
show_error_codes = true
check_untyped_defs = true
ignore_missing_imports = true
implicit_optional = true
implicit_reexport = true
files = [
  "stopes/"
]
exclude = [
  "stopes/modules/bitext/mining/mine_bitext_indexes_utils.py",
  "stopes/pipelines/distillation/distillation_pipeline.py",
  "stopes/eval/blaser/model.py",
  "stopes/pipelines/filtering/filter.py",
  "stopes/modules/preprocess/split_in_shards.py",
  "stopes/modules/bitext/mining/merge_shards.py",
  "stopes/eval/blaser/train.py",
  "stopes/modules/speech/shas/shas.py",
  "stopes/utils/tts_preprocessing/cmn.py",
  "stopes/pipelines/bitext/shard_and_shuffle.py",
  "stopes/pipelines/prepare_data/prepare_data.py",
  "stopes/pipelines/monolingual/dedup_files.py",
  "stopes/modules/bitext/mining/mine_bitext_sentences_utils.py",
  "stopes/eval/blaser/score.py",
  "stopes/eval/alti/alignment/align.py",
  "stopes/modules/tests/test_mine_index_utils.py",
  "stopes/modules/preprocess/laser_sentence_encoder.py",
  "stopes/utils/embedding_utils.py",
  "stopes/pipelines/monolingual/monolingual_pipeline.py",
  "stopes/modules/tests/test_populate_index_port.py",
  "stopes/modules/speech/whisper.py",
  "stopes/core/jobs_registry/registry.py",
  "stopes/pipelines/prepare_data/dedup_sharding.py",
  "stopes/pipelines/filtering/scripts/populate_data_conf.py",
  "stopes/pipelines/distillation/distillation_bitext_processor.py",
  "stopes/pipelines/bitext/ExtractMetaLineProc.py",
  "stopes/modules/tests/test_split_merge_langs.py",
  "stopes/core/jobs_registry/submitit_slurm_job.py",
  "stopes/pipelines/prepare_data/validate.py",
  "stopes/pipelines/monolingual/monolingual_line_processor.py",
  "stopes/pipelines/filtering/filters/lid.py",
  "stopes/modules/preprocess/multiproc_bitext_processor.py",
  "stopes/modules/evaluation/generate_multi_bleu_detok_module.py",
  "stopes/modules/bitext/indexing/populate_faiss_index.py",
  "stopes/utils/tts_preprocessing/numbers/__init__.py",
  "stopes/modules/translation/fairseq_generate.py",
  "stopes/modules/speech/shas/data.py",
  "stopes/eval/alti/wrappers/transformer_wrapper.py",
  "stopes/eval/alti/wrappers/multilingual_transformer_wrapper.py",
  "stopes/eval/alti/alti_metrics/nllb_alti_detector.py",
  "stopes/core/jobs_registry/stopes_job.py",
  "stopes/pipelines/translate/translation_pipeline.py",
  "stopes/pipelines/prepare_data/build_vocab.py",
  "stopes/pipelines/eval/eval_blaser.py",
  "stopes/pipelines/bitext/dedup_local_and_global.py",
  "stopes/pipelines/bitext/bitext_eval.py",
  "stopes/modules/tests/test_embedding_utils.py",
  "stopes/modules/preprocess/mining_speech_encoder.py",
  "stopes/modules/bitext/mining/calculate_distances.py",
  "stopes/utils/tts_preprocessing/cleaners.py",
  "stopes/pipelines/monolingual/utils/predict_lid.py",
  "stopes/pipelines/bitext/nmt_bitext_eval.py",
  "stopes/modules/tests/test_text_input.py",
  "stopes/modules/preprocess/multiproc_line_processor.py",
  "stopes/modules/bitext/indexing/sample_embedding_module.py",
  "stopes/ust_common/evaluation.py",
  "stopes/ust_common/text/numbers.py",
  "stopes/ust_common/tabulation.py",
  "stopes/ust_common/text/cn_tn.py",
  "stopes/ust_common/agg_results.py",
  "stopes/ust_common/sweep_utils.py",
  "stopes/ust_common/lib/audio.py",
  "stopes/ust_common/lib/manifests.py",
  "stopes/ust_common/lib/lpc.py",
  "stopes/ust_common/generation/asr_utils.py",
  "stopes/ust_common/viewer/notebook.py",
  "stopes/ust_common/lib/webrtc_vad.py",
  "stopes/ust_common/lib/__init__.py",
  "stopes/ust_common/generation/tts_utils.py",
  "stopes/ust_common/sweep/slurm.py",
  "stopes/ust_common/sweep/fblearner.py",
  "stopes/ust_common/lib/f0.py",
  "stopes/ust_common/generation/vocoder_utils/vocoder.py",
  "stopes/ust_common/utils/model_export.py",
]

[tool.pytest.ini_options]
minversion = "6.0"
testpaths = ["stopes"]
python_files = [
  "test_*.py",
  "monolingual/utils/*.py"
]
asyncio_mode = "auto"
norecursedirs = [
  "ust/*",
  "stopes/utils/aligner_utils",
  "stopes/eval/local_prosody/unity2_forced_aligner_f1",
]
asyncio_default_fixture_loop_scope = "session"
