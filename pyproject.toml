[build-system]
requires = ["setuptools>=61.0.0,<81.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "stopes"
readme = "README.md"
authors = [{name = "Facebook AI Research"}]
requires-python = ">=3.9"
version = "2.2.1"
description = "A library for preparing data for machine translation research."
dependencies = [
  "hydra-core>=1.2.0",
  "joblib",
  "submitit>=1.4.5",
  "tqdm",
  "pyarrow>=16.1.0"
]
classifiers=[
    "License :: OSI Approved :: MIT License",
    "Topic :: Scientific/Engineering",
    "Development Status :: 4 - Beta",
]

[project.urls]
Source = "https://github.com/facebookresearch/stopes"
Tracker = "https://github.com/facebookresearch/stopes/issues"

[project.optional-dependencies]
text = [
    "numpy==1.26.4",
    "pandas",
    "scipy",
    "torch",
    "transformers",
    "nltk",
    "xxhash",
    "sacremoses",
    "sentencepiece",
    "botok",
    "emoji",
    "hasami",
    "indic-nlp-library",
    "khmer-nltk",
    "laonlp",
    "pythainlp",
    "sentence_splitter",
]
speech = [
    "numpy==1.26.4",
    "pandas",
    "scipy",
    "torch",
    "transformers",
    "nltk",
    "xxhash",
    "sacremoses",
    "sentencepiece",
    "demucs",
    "encodec",
    "ipapy",
    "librosa",
    "num2words",
    "numba",
    "phonemizer",
    "syllables",
    "tnkeeh",
    "torchaudio",
    "unidecode",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper",
    "s3prl",
    "sentence_transformers",
]
training = [
    "fairscale",
    "omegaconf",
    "einops",
]
evaluation = [
    "fairscale",
    "omegaconf",
    "einops",
]
mining-cpu = [
    "numpy==1.26.4",
    "pandas",
    "scipy",
    "torch",
    "transformers",
    "nltk",
    "xxhash",
    "sacremoses",
    "sentencepiece",
    "beautifulsoup4",
    "func_argparse",
    "requests",
    "scikit-learn",
    "faiss-cpu",
]
mining-gpu = [
    "numpy==1.26.4",
    "pandas",
    "scipy",
    "torch",
    "transformers",
    "nltk",
    "xxhash",
    "sacremoses",
    "sentencepiece",
    "beautifulsoup4",
    "func_argparse",
    "requests",
    "scikit-learn",
    "faiss @ git+https://github.com/YaronKoresh/faiss/releases/download/6470b8d/faiss-1.12.0-py3-none-manylinux_2_35_x86_64.whl",
]
inference-cpu = [
    "numpy==1.26.4",
    "pandas",
    "scipy",
    "torch",
    "transformers",
    "nltk",
    "xxhash",
    "sacremoses",
    "sentencepiece",
    "botok",
    "emoji",
    "hasami",
    "indic-nlp-library",
    "khmer-nltk",
    "laonlp",
    "pythainlp",
    "sentence_splitter",
    "demucs",
    "encodec",
    "ipapy",
    "librosa",
    "num2words",
    "numba",
    "phonemizer",
    "syllables",
    "tnkeeh",
    "torchaudio",
    "unidecode",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper",
    "s3prl",
    "sentence_transformers",
    "beautifulsoup4",
    "func_argparse",
    "requests",
    "scikit-learn",
    "faiss-cpu",
    "sonar-space",
]
inference-gpu = [
    "numpy==1.26.4",
    "pandas",
    "scipy",
    "torch",
    "transformers",
    "nltk",
    "xxhash",
    "sacremoses",
    "sentencepiece",
    "botok",
    "emoji",
    "hasami",
    "indic-nlp-library",
    "khmer-nltk",
    "laonlp",
    "pythainlp",
    "sentence_splitter",
    "demucs",
    "encodec",
    "ipapy",
    "librosa",
    "num2words",
    "numba",
    "phonemizer",
    "syllables",
    "tnkeeh",
    "torchaudio",
    "unidecode",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper",
    "s3prl",
    "sentence_transformers",
    "beautifulsoup4",
    "func_argparse",
    "requests",
    "scikit-learn",
    "faiss @ git+https://github.com/YaronKoresh/faiss/releases/download/6470b8d/faiss-1.12.0-py3-none-manylinux_2_35_x86_64.whl",
    "sonar-space",
]
training-cpu = [
    "numpy==1.26.4",
    "pandas",
    "scipy",
    "torch",
    "transformers",
    "nltk",
    "xxhash",
    "sacremoses",
    "sentencepiece",
    "botok",
    "emoji",
    "hasami",
    "indic-nlp-library",
    "khmer-nltk",
    "laonlp",
    "pythainlp",
    "sentence_splitter",
    "demucs",
    "encodec",
    "ipapy",
    "librosa",
    "num2words",
    "numba",
    "phonemizer",
    "syllables",
    "tnkeeh",
    "torchaudio",
    "unidecode",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper",
    "s3prl",
    "sentence_transformers",
    "beautifulsoup4",
    "func_argparse",
    "requests",
    "scikit-learn",
    "faiss-cpu",
    "sonar-space",
    "fairscale",
    "omegaconf",
    "einops",
]
training-gpu = [
    "numpy==1.26.4",
    "pandas",
    "scipy",
    "torch",
    "transformers",
    "nltk",
    "xxhash",
    "sacremoses",
    "sentencepiece",
    "botok",
    "emoji",
    "hasami",
    "indic-nlp-library",
    "khmer-nltk",
    "laonlp",
    "pythainlp",
    "sentence_splitter",
    "demucs",
    "encodec",
    "ipapy",
    "librosa",
    "num2words",
    "numba",
    "phonemizer",
    "syllables",
    "tnkeeh",
    "torchaudio",
    "unidecode",
    "fairseq @ git+https://github.com/YaronKoresh/fairseq.git",
    "openai-whisper",
    "s3prl",
    "sentence_transformers",
    "beautifulsoup4",
    "func_argparse",
    "requests",
    "scikit-learn",
    "faiss @ git+https://github.com/YaronKoresh/faiss/releases/download/6470b8d/faiss-1.12.0-py3-none-manylinux_2_35_x86_64.whl",
    "sonar-space",
    "fairscale",
    "omegaconf",
    "einops",
]
dev = [
    "coverage[toml]",
    "flit",
    "mypy",
    "pytest",
    "pytest-asyncio",
    "pytest-cov",
    "ruff",
    "types-emoji",
    "types-PyYAML",
    "types-requests",
]

[tool.ruff]
line-length = 88
select = ["I"]

[tool.ruff.lint]
select = ["E", "F", "W", "C90", "I"]
ignore = []

[tool.mypy]
python_version = "3.9"
show_error_codes = true
check_untyped_defs = true
ignore_missing_imports = true
implicit_optional = true
implicit_reexport = true
files = [
  "stopes/"
]
exclude = [
  "stopes/modules/bitext/mining/mine_bitext_indexes_utils.py",
  "stopes/pipelines/distillation/distillation_pipeline.py",
  "stopes/eval/blaser/model.py",
  "stopes/pipelines/filtering/filter.py",
  "stopes/modules/preprocess/split_in_shards.py",
  "stopes/modules/bitext/mining/merge_shards.py",
  "stopes/eval/blaser/train.py",
  "stopes/modules/speech/shas/shas.py",
  "stopes/utils/tts_preprocessing/cmn.py",
  "stopes/pipelines/bitext/shard_and_shuffle.py",
  "stopes/pipelines/prepare_data/prepare_data.py",
  "stopes/pipelines/monolingual/dedup_files.py",
  "stopes/modules/bitext/mining/mine_bitext_sentences_utils.py",
  "stopes/eval/blaser/score.py",
  "stopes/eval/alti/alignment/align.py",
  "stopes/modules/tests/test_mine_index_utils.py",
  "stopes/modules/preprocess/laser_sentence_encoder.py",
  "stopes/utils/embedding_utils.py",
  "stopes/pipelines/monolingual/monolingual_pipeline.py",
  "stopes/modules/tests/test_populate_index_port.py",
  "stopes/modules/speech/whisper.py",
  "stopes/core/jobs_registry/registry.py",
  "stopes/pipelines/prepare_data/dedup_sharding.py",
  "stopes/pipelines/filtering/scripts/populate_data_conf.py",
  "stopes/pipelines/distillation/distillation_bitext_processor.py",
  "stopes/pipelines/bitext/ExtractMetaLineProc.py",
  "stopes/modules/tests/test_split_merge_langs.py",
  "stopes/core/jobs_registry/submitit_slurm_job.py",
  "stopes/pipelines/prepare_data/validate.py",
  "stopes/pipelines/monolingual/monolingual_line_processor.py",
  "stopes/pipelines/filtering/filters/lid.py",
  "stopes/modules/preprocess/multiproc_bitext_processor.py",
  "stopes/modules/evaluation/generate_multi_bleu_detok_module.py",
  "stopes/modules/bitext/indexing/populate_faiss_index.py",
  "stopes/utils/tts_preprocessing/numbers/__init__.py",
  "stopes/modules/translation/fairseq_generate.py",
  "stopes/modules/speech/shas/data.py",
  "stopes/eval/alti/wrappers/transformer_wrapper.py",
  "stopes/eval/alti/wrappers/multilingual_transformer_wrapper.py",
  "stopes/eval/alti/alti_metrics/nllb_alti_detector.py",
  "stopes/core/jobs_registry/stopes_job.py",
  "stopes/pipelines/translate/translation_pipeline.py",
  "stopes/pipelines/prepare_data/build_vocab.py",
  "stopes/pipelines/eval/eval_blaser.py",
  "stopes/pipelines/bitext/dedup_local_and_global.py",
  "stopes/pipelines/bitext/bitext_eval.py",
  "stopes/modules/tests/test_embedding_utils.py",
  "stopes/modules/preprocess/mining_speech_encoder.py",
  "stopes/modules/bitext/mining/calculate_distances.py",
  "stopes/utils/tts_preprocessing/cleaners.py",
  "stopes/pipelines/monolingual/utils/predict_lid.py",
  "stopes/pipelines/bitext/nmt_bitext_eval.py",
  "stopes/modules/tests/test_text_input.py",
  "stopes/modules/preprocess/multiproc_line_processor.py",
  "stopes/modules/bitext/indexing/sample_embedding_module.py",
  "stopes/ust_common/evaluation.py",
  "stopes/ust_common/text/numbers.py",
  "stopes/ust_common/tabulation.py",
  "stopes/ust_common/text/cn_tn.py",
  "stopes/ust_common/agg_results.py",
  "stopes/ust_common/sweep_utils.py",
  "stopes/ust_common/lib/audio.py",
  "stopes/ust_common/lib/manifests.py",
  "stopes/ust_common/lib/lpc.py",
  "stopes/ust_common/generation/asr_utils.py",
  "stopes/ust_common/viewer/notebook.py",
  "stopes/ust_common/lib/webrtc_vad.py",
  "stopes/ust_common/lib/__init__.py",
  "stopes/ust_common/generation/tts_utils.py",
  "stopes/ust_common/sweep/slurm.py",
  "stopes/ust_common/sweep/fblearner.py",
  "stopes/ust_common/lib/f0.py",
  "stopes/ust_common/generation/vocoder_utils/vocoder.py",
  "stopes/ust_common/utils/model_export.py",
]

[tool.pytest.ini_options]
minversion = "6.0"
testpaths = ["stopes"]
python_files = [
  "test_*.py",
  "monolingual/utils/*.py"
]
asyncio_mode = "auto"
norecursedirs = [
  "ust/*",
  "stopes/utils/aligner_utils",
  "stopes/eval/local_prosody/unity2_forced_aligner_f1",
]
asyncio_default_fixture_loop_scope = "session"
